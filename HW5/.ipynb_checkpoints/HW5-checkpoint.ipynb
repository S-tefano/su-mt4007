{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761b34b-81de-4c23-9621-4cd22e283e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7424cf-6c05-41fe-95bc-86fa0c564bc9",
   "metadata": {},
   "source": [
    "In this homework, I will complete two tasks: working with a REST API and web scraping.\n",
    "\n",
    "For the REST API part, I will fetch data in JSON format from the Nobel Prize API, specifically focusing on the Physics category. I will extract all prize motivations, clean the text by removing stop words and unnecessary spaces, and visualize the word frequencies using a word cloud.\n",
    "\n",
    "In the web scraping part, I will scrape book data from Books to Scrape. I will collect information such as UPC, title, price, and rating for the books on the first three pages (60 books in total) and organize the data into a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a834fae-1912-4fda-a79e-aefbe162d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from the Nobel Prize API\n",
    "response = requests.get('http://api.nobelprize.org/2.1/laureates?limit=300&nobelPrizeYear=1901&yearTo=2024&nobelPrizeCategory=phy')\n",
    "data = response.json()\n",
    "\n",
    "# Collect motivations\n",
    "motivations = []\n",
    "for laureate in data.get('laureates', []):\n",
    "    for prize in laureate.get('nobelPrizes', []):\n",
    "        motivation = prize.get('motivation', {}).get('en', '')\n",
    "        if motivation:\n",
    "            motivations.append(motivation)\n",
    "\n",
    "# Combine motivations into a single string\n",
    "text = ' '.join(motivations)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=set(WordCloud().stopwords)).generate(text)\n",
    "\n",
    "print(\"Here is the word cloud:\")\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6de1f-13e0-4555-a05a-1b24eac3710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
    "\n",
    "# Function to fetch book details\n",
    "def fetch_book_details(book_url):\n",
    "    response = requests.get(book_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {book_url}\")\n",
    "        return \"Not Found\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    upc_tag = soup.find('th', text=\"UPC\")\n",
    "    if upc_tag:\n",
    "        upc = upc_tag.find_next_sibling('td').text\n",
    "    else:\n",
    "        upc = \"Not Found\"\n",
    "    return upc\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_number):\n",
    "    url = base_url.format(page_number)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    data = []\n",
    "    for book in books:\n",
    "        title = book.h3.a['title']\n",
    "        price = book.find('p', class_='price_color').text\n",
    "        rating = book.p['class'][1]\n",
    "        book_url = \"https://books.toscrape.com/catalogue/\" + book.h3.a['href'].lstrip('./')\n",
    "        upc = fetch_book_details(book_url)\n",
    "        data.append([upc, title, price, rating])\n",
    "    return data\n",
    "\n",
    "# Scrape all pages (1-3)\n",
    "all_data = []\n",
    "for page in range(1, 4):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    all_data.extend(scrape_page(page))\n",
    "\n",
    "# Create DataFrame and save as CSV\n",
    "columns = ['upc', 'title', 'price', 'rating']\n",
    "df = pd.DataFrame(all_data, columns=columns)\n",
    "df.to_csv('books_scraped.csv', index=False)\n",
    "print(\"Here are the results:\")\n",
    "# Display DataFrame\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
